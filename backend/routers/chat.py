# backend/routers/chat.py
import os
import logging
from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
from sqlalchemy.orm import Session
from dotenv import load_dotenv
from huggingface_hub import InferenceClient
from sentence_transformers import SentenceTransformer
from sqlalchemy import text
from openai import OpenAI
from backend.database import get_db
from backend.models.invoice_model import Invoice
from sqlalchemy import or_

# ================================================================
# âš™ï¸ Setup
# ================================================================
load_dotenv()
router = APIRouter(prefix="/chat", tags=["Chat"])
logger = logging.getLogger("backend.chat")

HF_TOKEN = os.getenv("HF_TOKEN")

# Clients
hf_client = InferenceClient(api_key=HF_TOKEN)
llm_client = OpenAI(
    base_url="https://router.huggingface.co/v1",
    api_key=HF_TOKEN,
)

# Local embedding model (fallback)
local_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")


# ================================================================
# ğŸ§  Helper Classes & Functions
# ================================================================
class ChatRequest(BaseModel):
    question: str
    top_k: int = 3


def is_aggregation_question(question: str) -> bool:
    """Detects if the question involves aggregation (sum, count, etc.)"""
    keywords = ["sum", "total", "average", "count", "how many", "ÙƒÙ…", "Ù…Ø¬Ù…ÙˆØ¹", "Ø¥Ø¬Ù…Ø§Ù„ÙŠ"]
    q_lower = question.lower()
    return any(k in q_lower for k in keywords)


def fix_sql_casts(sql_query: str) -> str:
    """Casts numeric columns to FLOAT for math operations"""
    numeric_fields = [
        "subtotal", "tax", "total_amount", "grand_total",
        "discounts", "amount_paid"
    ]
    for col in numeric_fields:
        sql_query = sql_query.replace(col, f"CAST({col} AS FLOAT)")
    return sql_query


# ================================================================
# ğŸ’¬ Endpoint: /chat/ask
# ================================================================
@router.post("/ask")
async def ask_chat(request: ChatRequest, db: Session = Depends(get_db)):
    try:
        # ğŸ§® Mode 1 â€” Aggregation / SQL
        if is_aggregation_question(request.question):
            logger.info("ğŸ§® SQL Mode - Aggregation detected")

            completion = llm_client.chat.completions.create(
                model="meta-llama/Meta-Llama-3-8B-Instruct:novita",
                messages=[
                    {
                        "role": "system",
                        "content": (
                            "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„ÙÙˆØ§ØªÙŠØ± Ø¥Ù„Ù‰ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª SQL ØµØ­ÙŠØ­Ø©.\n\n"
                            "Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„ØªØ§Ù„ÙŠØ©:\n\n"
                            "1. Ø¬Ø¯ÙˆÙ„ invoices:\n"
                            "   - id, record, invoice_number, invoice_date, vendor, tax_number, cashier, branch, phone\n"
                            "   - subtotal, tax, total_amount, grand_total, discounts, payment_method, amount_paid\n"
                            "   - ticket_number, category, created_at, ai_insight\n\n"
                            "2. Ø¬Ø¯ÙˆÙ„ items:\n"
                            "   - id, invoice_id, description, quantity, unit_price, total\n\n"
                            "3. Ø¬Ø¯ÙˆÙ„ invoice_embeddings:\n"
                            "   - id, invoice_id, embedding\n\n"
                            "Ù‚ÙˆØ§Ø¹Ø¯ Ù…Ù‡Ù…Ø©:\n"
                            "- Ø£Ø¹Ø¯ ÙÙ‚Ø· Ø§Ø³ØªØ¹Ù„Ø§Ù… SQL ÙˆØ§Ø­Ø¯ Ù†Ø¸ÙŠÙ Ø¨Ø¯ÙˆÙ† Ø´Ø±Ø­\n"
                            "- Ø§Ø³ØªØ®Ø¯Ù… CAST Ù„Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ©\n"
                            "- total_amount Ùˆ subtotal Ùˆ tax Ù†ÙˆØ¹Ù‡Ø§ TEXT - Ø§Ø³ØªØ®Ø¯Ù… CAST(column AS FLOAT)\n"
                            "- category Ù†ÙˆØ¹Ù‡Ø§ JSON - Ø§Ø³ØªØ®Ø¯Ù… json_extract Ø¥Ø°Ø§ Ø§Ø­ØªØ¬Øª\n"
                            "- Ù„Ø§ ØªØ¶Ø¹ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø£Ùˆ ØªØ¹Ù„ÙŠÙ‚Ø§Øª ÙÙŠ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…"
                        ),
                    },
                    {"role": "user", "content": request.question},
                ],
            )

            sql_query = completion.choices[0].message.content.strip()
            sql_query = fix_sql_casts(sql_query)
            logger.info(f"ğŸ“ Generated SQL:\n{sql_query}")

            try:
                results = db.execute(text(sql_query)).fetchall()
                
                # Format Arabic response
                if results and len(results) == 1 and len(results[0]) == 1:
                    val = results[0][0]
                    # Format number with Arabic context
                    if isinstance(val, (int, float)):
                        return {"answer": f"Ø§Ù„Ù†ØªÙŠØ¬Ø©: **{val:,.2f}** {'Ø±ÙŠØ§Ù„' if 'Ù…Ø¨Ù„Øº' in request.question or 'Ø¥Ù†ÙØ§Ù‚' in request.question or 'Ø¥Ø¬Ù…Ø§Ù„ÙŠ' in request.question else ''}"}
                    return {"answer": f"Ø§Ù„Ù†ØªÙŠØ¬Ø©: **{val}**"}
                elif not results:
                    return {"answer": "Ù„Ù… Ø£Ø¬Ø¯ Ø£ÙŠ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ·Ø§Ø¨Ù‚ Ø³Ø¤Ø§Ù„Ùƒ."}
                else:
                    # Multiple results - format nicely
                    formatted_results = []
                    for row in results[:10]:  # Limit to 10 results
                        formatted_results.append(" | ".join([str(r) for r in row]))
                    
                    result_text = "\n".join(formatted_results)
                    return {"answer": f"Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ({len(results)} Ø³Ø¬Ù„):\n\n{result_text}"}
                    
            except Exception as sql_err:
                logger.error(f"âŒ SQL execution failed: {sql_err}")
                return {
                    "answer": f"Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù….\n\nØ§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {sql_query}\n\nØ§Ù„Ø®Ø·Ø£: {str(sql_err)}"
                }

        # ğŸ” Mode 2 â€” Semantic / RAG
        try:
            response = hf_client.post(
                path="https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2",
                json={"inputs": [request.question]},
            )
            emb = response.json()
            vector = [float(x) for x in emb[0]]
            logger.info("âœ… Got embedding from Hugging Face API")

        except Exception as api_err:
            logger.warning(f"âš ï¸ HF API failed, using local embedding: {api_err}")
            vector = local_model.encode([request.question])[0].tolist()

        # ğŸ” Mode 2 â€” Direct Retrieval (Vendor/Type Search)
        # Check if question is asking for specific vendor or invoice type
        retrieval_keywords = ['ÙØ§ØªÙˆØ±Ø©', 'ÙÙˆØ§ØªÙŠØ±', 'Ø£Ø±Ù†ÙŠ', 'Ø£Ø±Ø³Ù„', 'Ø§Ø¹Ø±Ø¶', 'ÙˆÙŠÙ†']
        is_retrieval = any(keyword in request.question for keyword in retrieval_keywords)
        
        if is_retrieval:
            logger.info("ğŸ–¼ï¸ Retrieval Mode - Looking for specific invoices")
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù…Ù† Ø§Ù„Ø³Ø¤Ø§Ù„
            keywords = request.question.lower().split()
            direct_results = []
            
            for keyword in keywords:
                if len(keyword) > 2:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
                    found = db.query(Invoice).filter(
                        or_(
                            Invoice.vendor.ilike(f"%{keyword}%"),
                            Invoice.invoice_type.ilike(f"%{keyword}%")
                        )
                    ).limit(5).all()
                    
                    if found:
                        direct_results.extend(found)
                        break  # ÙˆØ¬Ø¯Ù†Ø§ Ù†ØªØ§Ø¦Ø¬ØŒ Ù†ÙˆÙ‚Ù Ø§Ù„Ø¨Ø­Ø«
            
            # Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù†Ø§ Ù†ØªØ§Ø¦Ø¬ Ù…Ø¨Ø§Ø´Ø±Ø©ØŒ Ù†Ø³ØªØ®Ø¯Ù…Ù‡Ø§
            if direct_results:
                logger.info(f"ğŸ¯ Found {len(direct_results)} invoices")
                
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„Ù€ JSON Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
                invoices_data = []
                context_lines = []
                
                # Get SUPABASE_URL for building image URLs
                supabase_url = os.getenv("SUPABASE_URL", "https://pcktfzshbxaljkbedrar.supabase.co")
                
                for inv in direct_results[:5]:  # Ù†Ø£Ø®Ø° Ø£ÙˆÙ„ 5 ÙÙˆØ§ØªÙŠØ±
                    date_str = str(inv.invoice_date.strftime("%Y-%m-%d")) if inv.invoice_date else "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
                    
                    # Use stored image_url or fallback
                    image_url = inv.image_url or f"{supabase_url}/storage/v1/object/public/invoices/invoice_{inv.id}.jpg"
                    
                    invoices_data.append({
                        "id": inv.id,
                        "vendor": inv.vendor or "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ",
                        "invoice_number": inv.invoice_number or "ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                        "invoice_type": inv.invoice_type or "ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                        "date": date_str,
                        "total": str(inv.total_amount) if inv.total_amount else "0.00",
                        "tax": str(inv.tax) if inv.tax else "0.00",
                        "payment_method": inv.payment_method or "ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
                        "image_url": image_url,
                        "category": inv.category or "{}",
                    })
                    
                    context_lines.append(
                        f"ÙØ§ØªÙˆØ±Ø© Ø±Ù‚Ù… {inv.id} Ù…Ù† {inv.vendor or 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ'} Ø¨ØªØ§Ø±ÙŠØ® {date_str} Ø¨Ù…Ø¨Ù„Øº {inv.total_amount or 0.0} Ø±ÙŠØ§Ù„"
                    )
                
                context_text = "\n".join(context_lines)
                
                # ğŸ’¬ Generate final natural language answer in Arabic
                completion = llm_client.chat.completions.create(
                    model="meta-llama/Meta-Llama-3-8B-Instruct:novita",
                    messages=[
                        {
                            "role": "system",
                            "content": (
                                "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙŠØ¬ÙŠØ¨ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„ÙÙˆØ§ØªÙŠØ± Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.\n"
                                "Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙˆØ·Ø¨ÙŠØ¹ÙŠ.\n"
                                "Ø§Ø°ÙƒØ± Ø¹Ø¯Ø¯ Ø§Ù„ÙÙˆØ§ØªÙŠØ± ÙˆØ§Ù„Ù…Ø¨Ø§Ù„Øº ÙˆØ§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ù‡Ù…Ø©."
                            )
                        },
                        {
                            "role": "user",
                            "content": f"Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {request.question}\n\nØ§Ù„ÙÙˆØ§ØªÙŠØ± Ø§Ù„Ù…ØªØ§Ø­Ø©:\n{context_text}"
                        }
                    ],
                )
                
                ai_answer = completion.choices[0].message.content.strip()
                return {
                    "answer": ai_answer,
                    "invoices": invoices_data  # Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„ÙÙˆØ§ØªÙŠØ± Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
                }
        
        # ğŸ” Mode 3 â€” RAG (Semantic Search for general questions)
        logger.info("ğŸ“„ RAG Mode - Semantic search with embeddings")
        
        query = text("""
            SELECT i.id, i.vendor, i.subtotal, i.total_amount, i.invoice_date, i.category,
                   e.embedding <-> CAST(:vec AS vector) AS distance
            FROM invoices i
            JOIN invoice_embeddings e ON i.id = e.invoice_id
            ORDER BY distance ASC
            LIMIT :top_k;
        """)
        results = db.execute(query, {"vec": vector, "top_k": request.top_k}).fetchall()

        if not results:
            return {"answer": "Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ ÙÙˆØ§ØªÙŠØ± Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø³Ø¤Ø§Ù„Ùƒ. Ø¬Ø±Ø¨ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø£Ùˆ Ø§Ø³Ø£Ù„ Ø¹Ù† Ù…ØªØ¬Ø± Ù…Ø­Ø¯Ø¯."}

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        context_lines = []
        for r in results:
            date_str = r.invoice_date.strftime("%Y-%m-%d") if r.invoice_date else "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
            vendor = r.vendor or "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"
            total = float(r.total_amount) if r.total_amount else 0.0
            
            context_lines.append(
                f"ÙØ§ØªÙˆØ±Ø© Ø±Ù‚Ù… {r.id} Ù…Ù† {vendor} Ø¨ØªØ§Ø±ÙŠØ® {date_str} Ø¨Ù…Ø¨Ù„Øº {total:.2f} Ø±ÙŠØ§Ù„"
            )

        context_text = "\n".join(context_lines)

        # ğŸ’¬ Generate final natural language answer in Arabic
        completion = llm_client.chat.completions.create(
            model="meta-llama/Meta-Llama-3-8B-Instruct:novita",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙÙˆØ§ØªÙŠØ± ÙˆØ¥Ø¯Ø§Ø±ØªÙ‡Ø§.\n"
                        "Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­ ÙˆÙ…ÙÙŠØ¯ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.\n"
                        "ÙƒÙ† Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ ÙÙŠ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… ÙˆØ§Ø°ÙƒØ± Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ù‡Ù…Ø©.\n"
                        "Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ©ØŒ Ø§Ø°ÙƒØ± Ø°Ù„Ùƒ Ø¨ÙˆØ¶ÙˆØ­."
                    )
                },
                {
                    "role": "user",
                    "content": f"Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {request.question}\n\nØ§Ù„ÙÙˆØ§ØªÙŠØ± Ø§Ù„Ø£ÙƒØ«Ø± ØµÙ„Ø©:\n{context_text}"
                }
            ],
        )

        ai_answer = completion.choices[0].message.content.strip()
        return {"answer": ai_answer}

    except Exception as e:
        logger.error(f"âŒ Chat failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))
